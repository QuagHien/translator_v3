{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install gdown datasets evaluate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone -b eval https://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@github.com/nguyenvanthanhdat/translator.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME_OR_PATH=\"checkpoint-*\"\n",
    "MODEL_NAME_OR_PATH=\"google/flan-t5-large\"\n",
    "TOKEN_NAME='google/flan-t5-large'\n",
    "HF_TOKEN='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' \n",
    "DATA_TEST_PATH='presencesw/phomt_eval_20_256'\n",
    "SPLIT='test'\n",
    "# GDOWN_ID='1-1hVXhUpk-elUCKs6oMuxsNvIv9NFgdm' # 5K\n",
    "# GDOWN_ID='1RLLtLecCyoFx3yrSA5nuiWD_6qr_I0AA' # 15K\n",
    "GDOWN_ID='1q5wTJ6n4gpVSFM_ZVENDxSCjyyvHF2iN' # 20K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0,1 accelerate launch --gpu_ids 0,1 --num_processes=2 -m translator.eval \\\n",
    "    --model_name_or_path $MODEL_NAME_OR_PATH \\\n",
    "    --tokenizer_name_or_path $TOKEN_NAME \\\n",
    "    --hf_key $HF_TOKEN \\\n",
    "    --data_test_path $DATA_TEST_PATH \\\n",
    "    --split $SPLIT \\\n",
    "    --num_proc 2 \\\n",
    "    --batch_size 10 \\\n",
    "    --max_len 256 \\\n",
    "    --num_beams 3,4,5 \\\n",
    "    --hf_key $HF_TOKEN \\\n",
    "    --gdown_id $GDOWN_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datasets, evaluate\n",
    "os.chdir(\"eval\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "score = open(\"../score.txt\", \"w\")\n",
    "for file_txt in os.listdir(\".\"):\n",
    "    dataset = datasets.load_dataset(\"json\", file_txt, split='train')\n",
    "    try:\n",
    "        results = bleu.compute(predictions=dataset['predict'], references=dataset['label'])\n",
    "    except:\n",
    "        results = 0\n",
    "score.write(f\"{file_txt}: bleu - {results}\\n\")\n",
    "score.close()\n",
    "os.chdir(\"..\")\n",
    "os.system(\"zip -r result.zip eval\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
