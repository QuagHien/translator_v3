{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28561,"status":"ok","timestamp":1702874338985,"user":{"displayName":"Thành Đạt","userId":"12870977663865043761"},"user_tz":-420},"id":"S5Jvr8qt_3Y7","outputId":"1f776cd2-1c5d-4b8a-f974-003c1df83f79"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1349,"status":"ok","timestamp":1702874340329,"user":{"displayName":"Thành Đạt","userId":"12870977663865043761"},"user_tz":-420},"id":"dgfgmAXL_4jW","outputId":"009780c8-efc4-497c-b97b-204330e2e2d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Project/translator\n"]}],"source":["%cd /content/drive/MyDrive/Project/translator"]},{"cell_type":"code","source":["!pip install accelerate deepspeed"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XCRMryTQN5gh","executionInfo":{"status":"ok","timestamp":1702874369527,"user_tz":-420,"elapsed":23915,"user":{"displayName":"Thành Đạt","userId":"12870977663865043761"}},"outputId":"a2bc6270-542e-4244-cf46-7321355c0c3c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting accelerate\n","  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting deepspeed\n","  Downloading deepspeed-0.12.5.tar.gz (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n","Collecting hjson (from deepspeed)\n","  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ninja (from deepspeed)\n","  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.10.13)\n","Collecting pynvml (from deepspeed)\n","  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed) (4.66.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Building wheels for collected packages: deepspeed\n","  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for deepspeed: filename=deepspeed-0.12.5-py3-none-any.whl size=1297449 sha256=cead1801f308ca577e7c669f4553b10a9ff931660b457fa218dddc7ddac0bc32\n","  Stored in directory: /root/.cache/pip/wheels/7d/4e/ca/24e93c09d21013b572826feb20cbef59b599f3d617bf075038\n","Successfully built deepspeed\n","Installing collected packages: ninja, hjson, pynvml, deepspeed, accelerate\n","Successfully installed accelerate-0.25.0 deepspeed-0.12.5 hjson-3.1.0 ninja-1.11.1.1 pynvml-11.5.0\n"]}]},{"cell_type":"code","source":["!accelerate config"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UMJA8m1yONZ4","executionInfo":{"status":"ok","timestamp":1702874420330,"user_tz":-420,"elapsed":21702,"user":{"displayName":"Thành Đạt","userId":"12870977663865043761"}},"outputId":"b77b7976-90bb-4d7d-f23a-349de0bc77fc"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\r----------------------------------------------------------------------------------------------------In which compute environment are you running?\n","Please input a choice index (starting from 0), and press enter\n"," ➔  \u001b[32mThis machine\u001b[0m\r\n","    AWS (Amazon SageMaker)\n","\u001b[2A\u001b[?25l\n","\u001b[32mThis machine\u001b[0m\n","----------------------------------------------------------------------------------------------------Which type of machine are you using?\n","Please input a choice index (starting from 0), and press enter\n"," ➔  \u001b[32mNo distributed training\u001b[0m\n","    multi-CPU\n","    multi-XPU\n","    multi-GPU\n","    multi-NPU\n","    TPU\n","\u001b[6A\u001b[?25l\u001b[?25hTraceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/menu/cursor.py\", line 63, in hide\n","    yield\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/menu/selection_menu.py\", line 132, in run\n","    choice = int(builtins.input())\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/accelerate\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\n","    args.func(args)\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/config/config.py\", line 67, in config_command\n","    config = get_user_input()\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/config/config.py\", line 40, in get_user_input\n","    config = get_cluster_input()\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/config/cluster.py\", line 49, in get_cluster_input\n","    distributed_type = _ask_options(\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/config/config_utils.py\", line 60, in _ask_options\n","    result = menu.run(default_choice=default)\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/menu/selection_menu.py\", line 128, in run\n","    with cursor.hide():\n","  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n","    self.gen.throw(typ, value, traceback)\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/menu/cursor.py\", line 65, in hide\n","    show_cursor()\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/menu/cursor.py\", line 55, in show_cursor\n","    sys.stdout.flush()\n","KeyboardInterrupt\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bgAROKWvANtp"},"outputs":[],"source":["%%capture\n","!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"iv1zuI26_hpx","outputId":"56e31b74-4cce-42e6-f8fa-5a29259c37bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-12-12 13:22:41.072848: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-12 13:22:41.072901: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-12 13:22:41.072945: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-12 13:22:43.063669: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-large/snapshots/50b7223e98fcd124b0cabb1ec81bc6324c7df107/config.json\n","Model config MT5Config {\n","  \"_name_or_path\": \"google/mt5-large\",\n","  \"architectures\": [\n","    \"MT5ForConditionalGeneration\"\n","  ],\n","  \"classifier_dropout\": 0.0,\n","  \"d_ff\": 2816,\n","  \"d_kv\": 64,\n","  \"d_model\": 1024,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"gelu_new\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"gated-gelu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": true,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"mt5\",\n","  \"num_decoder_layers\": 24,\n","  \"num_heads\": 16,\n","  \"num_layers\": 24,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"tie_word_embeddings\": false,\n","  \"tokenizer_class\": \"T5Tokenizer\",\n","  \"transformers_version\": \"4.35.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 250112\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-large/snapshots/50b7223e98fcd124b0cabb1ec81bc6324c7df107/config.json\n","Model config MT5Config {\n","  \"_name_or_path\": \"google/mt5-large\",\n","  \"architectures\": [\n","    \"MT5ForConditionalGeneration\"\n","  ],\n","  \"classifier_dropout\": 0.0,\n","  \"d_ff\": 2816,\n","  \"d_kv\": 64,\n","  \"d_model\": 1024,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"gelu_new\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"gated-gelu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": true,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"mt5\",\n","  \"num_decoder_layers\": 24,\n","  \"num_heads\": 16,\n","  \"num_layers\": 24,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"tie_word_embeddings\": false,\n","  \"tokenizer_class\": \"T5Tokenizer\",\n","  \"transformers_version\": \"4.35.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 250112\n","}\n","\n","loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google--mt5-large/snapshots/50b7223e98fcd124b0cabb1ec81bc6324c7df107/spiece.model\n","loading file tokenizer.json from cache at None\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--mt5-large/snapshots/50b7223e98fcd124b0cabb1ec81bc6324c7df107/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-large/snapshots/50b7223e98fcd124b0cabb1ec81bc6324c7df107/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-large/snapshots/50b7223e98fcd124b0cabb1ec81bc6324c7df107/config.json\n","Model config MT5Config {\n","  \"_name_or_path\": \"google/mt5-large\",\n","  \"architectures\": [\n","    \"MT5ForConditionalGeneration\"\n","  ],\n","  \"classifier_dropout\": 0.0,\n","  \"d_ff\": 2816,\n","  \"d_kv\": 64,\n","  \"d_model\": 1024,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"gelu_new\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"gated-gelu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": true,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"mt5\",\n","  \"num_decoder_layers\": 24,\n","  \"num_heads\": 16,\n","  \"num_layers\": 24,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"tie_word_embeddings\": false,\n","  \"tokenizer_class\": \"T5Tokenizer\",\n","  \"transformers_version\": \"4.35.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 250112\n","}\n","\n","You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-large/snapshots/50b7223e98fcd124b0cabb1ec81bc6324c7df107/config.json\n","Model config MT5Config {\n","  \"_name_or_path\": \"google/mt5-large\",\n","  \"architectures\": [\n","    \"MT5ForConditionalGeneration\"\n","  ],\n","  \"classifier_dropout\": 0.0,\n","  \"d_ff\": 2816,\n","  \"d_kv\": 64,\n","  \"d_model\": 1024,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"gelu_new\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"gated-gelu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": true,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"mt5\",\n","  \"num_decoder_layers\": 24,\n","  \"num_heads\": 16,\n","  \"num_layers\": 24,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"tie_word_embeddings\": false,\n","  \"tokenizer_class\": \"T5Tokenizer\",\n","  \"transformers_version\": \"4.35.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 250112\n","}\n","\n","/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-large/snapshots/50b7223e98fcd124b0cabb1ec81bc6324c7df107/config.json\n","Model config MT5Config {\n","  \"_name_or_path\": \"google/mt5-large\",\n","  \"architectures\": [\n","    \"MT5ForConditionalGeneration\"\n","  ],\n","  \"classifier_dropout\": 0.0,\n","  \"d_ff\": 2816,\n","  \"d_kv\": 64,\n","  \"d_model\": 1024,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"gelu_new\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"gated-gelu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": true,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"mt5\",\n","  \"num_decoder_layers\": 24,\n","  \"num_heads\": 16,\n","  \"num_layers\": 24,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"tie_word_embeddings\": false,\n","  \"tokenizer_class\": \"T5Tokenizer\",\n","  \"transformers_version\": \"4.35.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 250112\n","}\n","\n","Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--google--mt5-large/snapshots/50b7223e98fcd124b0cabb1ec81bc6324c7df107/pytorch_model.bin\n","Instantiating MT5ForConditionalGeneration model under default dtype torch.float16.\n","Generate config GenerationConfig {\n","  \"decoder_start_token_id\": 0,\n","  \"eos_token_id\": 1,\n","  \"pad_token_id\": 0\n","}\n","\n","Detected 4-bit loading: activating 4-bit loading for this model\n","All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n","\n","All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at google/mt5-large.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n","loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-large/snapshots/50b7223e98fcd124b0cabb1ec81bc6324c7df107/generation_config.json\n","Generate config GenerationConfig {\n","  \"decoder_start_token_id\": 0,\n","  \"eos_token_id\": 1,\n","  \"pad_token_id\": 0\n","}\n","\n","-------------------------------------------------- \n","\n","trainable params: 11151360 || all params: 951325696 || trainable%: 1.1721916108108574\n","-------------------------------------------------- \n","\n","Map: 100% 200/200 [00:00<00:00, 529.01 examples/s] \n","***** Running training *****\n","  Num examples = 1,800\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1,800\n","  Number of trainable parameters = 11,151,360\n","  0% 0/1800 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","  2% 30/1800 [00:23<18:14,  1.62it/s]"]}],"source":["MODEL_NAME_OR_PATH=\"google/mt5-large\"\n","OUTPUT_DIR='output'\n","BZ=4\n","\n","\n","!python -m translator.training \\\n","    --model_name_or_path $MODEL_NAME_OR_PATH \\\n","    --output_dir $OUTPUT_DIR \\\n","    --do_train \\\n","    --train_dir 'data/finetune/dataset_1000' \\\n","    --do_eval \\\n","    --valid_dir 'data/finetune/dataset_1000' \\\n","    --quantize \\\n","    --use_lora \\\n","    --per_device_train_batch_size $BZ \\\n","    --per_device_train_batch_size $BZ \\\n","    --num_train_epochs 4 \\\n","    --max_len 256 \\\n","    --save_steps 1000 \\\n","    --eval_steps 1000 \\\n","    --logging_steps 100 \\\n","    --evaluation_strategy 'steps' \\\n","    --step_by_step \\\n","    --overwrite_output_dir \\\n","    --load_best_model_at_end"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}